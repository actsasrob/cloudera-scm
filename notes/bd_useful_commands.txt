
Hadoop CLI:

yarn application -list -appStates FINISHED # Get list of apps that have finished
   Copy the application ID for the Spark application returned by the command above.
   Now run this command (where appId is the actual application ID).
yarn logs -applicationId <appId> | less

sudo find /data/nn/ -name "*<BLK_ID>*"  # Find actual file containing data block on DataNode
                                        # NOTE: Assume file system prefix of /data/nn.

# DFS Commands
# Use sudo -u <hdfs admin user> for these commands
sudo -u hdfs hdfs dfsadmin -safemode enter # Set filesystem to read-only mode. No replication or deletes.
hdfs dfsadmin -safemode leave
hdfs dfsadmin -safemode wait  # Command blocks until safemode exits. Useful for scripts
hdfs dfsadmin -saveNamespace # Save Namenode metadata to disk. Reset edits log.
                             # Note: Must be in safemode
hdfs dfsadmin -allowSnapshot <directory>  # Enable snapshots for directory
hdfs dfsadmin -report # Report information about DFS filesystem
hdfs hdfs dfsadmin -printTopology # Print HDFS cluster topology
hdfs dfsadmin -refreshNodes  # Re-read dfs.hosts and dfs.hosts.exclude files
hdfs fsck / -files -blocks -locations -racks # List info about HDFS files.
hdfs fsck / -file -block -locations -openforwrite # include files opened for write
hdfs fsck / -list-curruptfilesblocks # list of missing blocks and files they belong to
hdfs getconf -namenodes # List NameNodes for cluster

## HDFS encryption
hadoop key list # List encryption keys
hadoop key create mykey # Create new HDFS encryption key "mykey"
hdfs crypto -listZones # List encryption zones
# Create new HDFS encryption zone using key "mykey" and path "/test/encr/place"
hdfs crypto -createZone -keyName mykey -path /test/encr/place
hdfs fs -rm -r -f -skipTrash /test/encr/place # Remove an encryption zone

# Moving DAta Between encrypted and unencrypted directories
# It is not possible to use the hadoop fs -mv option to migrate data between encrypted and unencrypted paths
#   or from one encryption zone to another. Instead, a distcp must be run to copy the data.  The -skipcrccheck
#   -update options to distcp are required.

##distcp
# Copying data from one cluster to another
– hadoop distcp hdfs://cluster1_nn:8020/path/to/src hdfs://cluster2_nn:8020/path/to/dest
# Copying data within the same cluster
– hadoop distcp /path/to/src /path/to/dest
# Copying data from one cluster to another when the clusters are running different versions of Hadoop
– HA HDFS example using HApFS
– hadoop distcp hdfs://mycluster/path/to/src webhdfs://httpfs-svr:14000/path/to/dest
– Non-HA HDFS example using WebHDFS
– hadoop distcp hdfs://cluster1_nn:8020/path/to/src webhdfs://cluster2_nn:50070/path/to/dest

# yarn CLI commands:
yarn application –list all  # view all applicaHons on the cluster, including completed applications
yarn application –status <application_ID> # To display the status of an individual application
yarn application –kill <application_ID>
yarn application -list -appStates FINISHED | grep 'word count' # Look up application ID
yarn logs -applicationId application_1392918622651_0004  # View logs

## Cloudera Configuration:
– A subdirectory of: /var/run/cloudera-scm-agent/process/

# Cloudera Manager stores client configura1ons separately from service configurations
– Default location: /etc/hadoop/conf
# Cloudera Manager creates a “client configura1on file” zip archive of the configuration files containing service properties
– Each archive has the configuration files needed to access the service
– Example: a MapReduce client configura-on file contains copies of core-site.xml, hadoop-env.sh, hdfs-site.xml,log4j.properties and mapred-site.xml

# Cloudera Manager decouples serverand client configurations
– Server seSngs (e.g., NameNode,DataNode) are stored in/var/run/cloudera-scm-agent/process subdirectories
– Client seSngs are stored in /etc/hadoop subdirectories

## Cloudera Hadoop/HDFS Logs:

When YARN log aggregation is enabled: Container log files are moved from NodeManager hosts'
/var/log/hadoop-yarn/container directories to HDFS when the application completes
- Default HDFS directory: /tmp/logs
MapReduce jobs produce the following logs:
- ApplicationMaster log
- stdout, stderr, and syslog output for each Map and Reduce task
- Job configuration settings specified by the developer
- Counters
MapReduce Job History Server log directory Default: /var/log/hadoop-mapreduce

# Hive/Impala
create external table ncdc2 (record string) 
partitioned by (station string, year string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/user/rob/output1';

msck repair table ncdc2; # In Hive shell and run following command to make data in paritions visible:
refresh ncdc2; # Execute in Impala to make table data visible after running above command in Hive

Spark application history location
– Default location in HDFS: /user/spark/applicationHistory
– Find and view the logs from the command line:
– $ sudo –u hdfs hdfs dfs /user/spark/applicationHistory
– $ sudo –u hdfs hdfs dfs –cat /user/spark/application_<application_id>/EVENT_LOG_1
# Spark History Server log directory Default: /var/log/spark

# Hue Troubleshooting
Navigate to the Hue service configuration in Cloudera Manager
Search for Hue Service Environment Advanced Configuration Snippet (Safety Valve) and add the following:
DEBUG=true
DESKTOP_DEBUG=true
Save and restart Hue service
Collect the logs from /var/run/cloudera-scm-agent/process/<id>-hue-HUE_SERVER/logs where <id> is the most recently created. (Logs are not stored in /var/log/hue)

# Avro/Parquet
avro-tools tojson <file> # Display contents of Avro file in JSON
avro-tools totext <file> # Display contents of Avro file as text
parquet-tools dump <file> # Dump contents of Parquest file

# Kite Troubleshooting/examples
debug=true kite-dataset info  # Display environment variables used by Kite
debug=true kite-dataset info users   # Get info about 'users' table and env vars

#!/bin/bash
set -x
ZKHOSTS=scm1
# create partition strategy 
kite-dataset -v partition-config id:copy -s schema.avsc -o partition.json 
# create mapping-config 
kite-dataset -v mapping-config id:key customerName:common customerAddress:common -s schema.avsc \
     -p partition.json -o mapping.json 
# create dataset
kite-dataset create dataset:hbase:$ZKHOSTS/customers --use-hbase -s schema.avsc \
      --partition-by partition.json --mapping mapping.json 
      
#!/bin/bash
set -x
ZKHOSTS=scm1
kite-dataset csv-import customers1.csv  dataset:hbase:$ZKHOSTS/customers --no-header --delimiter '\t'

kite-dataset show dataset:hbase:scm1/customers
kite-dataset schema dataset:hbase:scm1/customers
kite-dataset flume-config --channel-type memory dataset:hbase:scm1/customers -o flume_kite.conf

# Access all application logs in Cloudera Manager
– From the YARN Applications page, choose “Collect diagnostics data”
– Options to “Download Result Data”, and view recent or full logs

## Network troubleshooting commands:
tcpdump cheat sheet:
tcpdump [ -AdDefIKlLnNOpqRStuUvxX ] [ expression ]

sudo tcpdump -tttt -nn -l 'host scm1 and hadoop-01-01' # Only traffic between hosts scm1 and hadoop-01-01
sudo tcpdump -tttt -nn -l 'host hadoop-01-01 and ( port 24000 or 8022 or 22 )'
tcpdump 'tcp[tcpflags] & (tcp-syn|tcp-fin) != 0 and not src and dst net localnet' # To print the start and 
             end packets (the SYN and FIN packets) of each TCP conversation that involves a non-local host.
tcpdump -D # List device interfaces
tcpdump -i eth1 # Capture only on interface eth1
tcpdump -w file.pcap # Capture packets to file file.pcap
tcpdump -r file.cap  # Read from file file.pcap.
tcpdump -l  # Make stdout line buffered.  Useful if you want to see the data while capturing it.  E.g.,
               "tcpdump  -l  |  tee dat" or "tcpdump  -l   > dat  &  tail  -f  dat".
tcpdump -n  # Don't convert host addresses to names.  This can be used to avoid DNS lookups.
tcpdump -nn # Don't convert protocol and port numbers etc. to names either.
       -t     Don't print a timestamp on each dump line.
       -tt    Print an unformatted timestamp on each dump line.
       -ttt   Print a delta (micro-second resolution) between current and previous line on each dump line.
       -tttt  Print a timestamp in default format proceeded by date on each dump line.
       -ttttt Print a delta (micro-second resolution) between current and first line on each dump line.
e.g.   sudo tcpdump -tttt -l src hadoop-01-01 | tee /tmp/tcpdump1.txt
tcpdump -XX # Print packet contents in Hex and ASCII. Use -A for just ASCII

Filter Options  (see 'man pcap-filter'):
      type   type qualifiers say what kind of thing the id name or number refers to.  
             Possible types are host, net , port and portrange.  E.g., 'host foo', 'net 128.3', 'port 20', 'por-
              trange 6000-6008'.  If there is no type qualifier, host is assumed.
       dir    dir qualifiers specify a particular transfer direction to and/or from id.  
              Possible directions are src, dst, src or dst, src and dst,  etc.
              E.g.,  'src  foo',  'dst  net 128.3', 'src or dst port ftp-data'.  
              If there is no dir qualifier, src or dst is assumed.
       proto  proto  qualifiers restrict the match to a particular protocol.  Possible protos are: 
                 ether, fddi, tr, wlan, ip, ip6, arp, rarp, decnet, tcp and udp.  E.g., 'ether src foo',
              If there is no proto qualifier, all protocols consistent with the  type  are  assumed.
              E.g., 'src foo' means '(ip or arp or rarp) src foo' (except the latter is not legal syntax), and 
                    'port 53' means '(tcp or udp) port 53'.

      Primitives may be combined using: A parenthesized group of primitives and operators (parentheses are 
              special to the Shell and must be escaped).
              Negation ('!' or 'not'),  Concatenation ('&&' or 'and'),  Alternation ('||' or 'or').
       If an identifier is given without a keyword, the most recent keyword is assumed.  For example,
            not host vs and ace is short for not host vs and host ace

tcpdump Flags:
    [S] - SYN (Start Connection)
    [.] - No Flag Set
    [S.]- Some tcpdump versions print SYN-ACK as 'S.'
    [P] - PSH (Push Data)
    [F] - FIN (Finish Connection)
    [R] - RST (Reset Connection)

    When a TCP connection is being initialized, there is a three way handshake (SYN SYN-ACK ACK) to sync up the connection. 
    After the handshake, packets are sent and ACK along the way. When the connection is being terminated, there is another 
     three way handshake (FIN ACK-FIN ACK) to close the connection.`

=============================================================================

netcat/nc commands:

netcat -z -v domain.com 1-1000 # Use nc to scan ports 1-1000
netcat -z -n -v 111.111.111.111 1-1000 # Scan on a given IP address
nc -z -v hadoop-01-01 50000-65535 2>&1 | grep -v "Connection refused" # Filter out failed ports
nc -l 2517    # Listen on port 2517
nc <host> 2517   # Connect to host on port 2517


# Display network activity for a running process
strace -p <pid> -f -e trace=network -s 10000

## AWS
# Get instance metadata
curl http://169.254.169.254/latest/meta-data/ # Get list of latest instance metadata
curl http://169.254.169.254/latest/meta-data/public-ipv4 # Get public IP address

## Misc:
sudo du --max-depth=1 --one-file-system --bytes /var/log | sort --numeric
sudo du --summarize --one-file-system --bytes /var/log/* | sort --numeric

# Cloudera Manager API:
curl -X POST -u admin:admin 'http://192.168.122.248:7180/api/v11/clusters/Cluster%201/commands/stop'
curl -u admin:admin 'http://192.168.122.248:7180/api/v11/commands/723'  # Get status for command 723
  
