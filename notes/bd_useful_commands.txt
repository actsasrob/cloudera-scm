
Hadoop CLI:

yarn application -list -appStates FINISHED # Get list of apps that have finished
   Copy the application ID for the Spark application returned by the command above.
   Now run this command (where appId is the actual application ID).
yarn logs -applicationId <appId> | less

sudo find /data/nn/ -name "*<BLK_ID>*"  # Find actual file containing data block on DataNode
                                        # NOTE: Assume file system prefix of /data/nn.

# DFS Commands
hdfs dfsadmin -safemode enter # Set filesystem to read-only mode. No replication or deletes.
hdfs dfsadmin -safemode leave
hdfs dfsadmin -safemode wait  # Command blocks until safemode exits. Useful for scripts
hdfs dfsadmin -saveNamespace # Save Namenode metadata to disk. Reset edits log.
                             # Note: Must be in safemode
hdfs dfsadmin -allowSnapshot <directory>  # Enable snapshots for directory
hdfs dfsadmin -report # Report information about DFS filesystem
hdfs dfsadmin -refreshNodes  # Re-read dfs.hosts and dfs.hosts.exclude files
hdfs fsck / -files -blocks -locations -racks # List info about HDFS files.

##distcp
# Copying data from one cluster to another
– hadoop distcp hdfs://cluster1_nn:8020/path/to/src hdfs://cluster2_nn:8020/path/to/dest
# Copying data within the same cluster
– hadoop distcp /path/to/src /path/to/dest
# Copying data from one cluster to another when the clusters are running different versions of Hadoop
– HA HDFS example using HApFS
– hadoop distcp hdfs://mycluster/path/to/src webhdfs://httpfs-svr:14000/path/to/dest
– Non-HA HDFS example using WebHDFS
– hadoop distcp hdfs://cluster1_nn:8020/path/to/src webhdfs://cluster2_nn:50070/path/to/dest

# yarn CLI commands:
yarn application –list all  # view all applicaHons on the cluster, including completed applications
yarn application –status <application_ID> # To display the status of an individual application
yarn application –kill <application_ID>
yarn application -list -appStates FINISHED | grep 'word count' # Look up application ID
yarn logs -applicationId application_1392918622651_0004  # View logs

## Cloudera Configuration:
– A subdirectory of: /var/run/cloudera-scm-agent/process/

# Cloudera Manager stores client configura1ons separately from service configurations
– Default location: /etc/hadoop/conf
# Cloudera Manager creates a “client configura1on file” zip archive of the configuration files containing service properties
– Each archive has the configuration files needed to access the service
– Example: a MapReduce client configura-on file contains copies of core-site.xml, hadoop-env.sh, hdfs-site.xml,log4j.properties and mapred-site.xml

# Cloudera Manager decouples serverand client configurations
– Server seSngs (e.g., NameNode,DataNode) are stored in/var/run/cloudera-scm-agent/process subdirectories
– Client seSngs are stored in /etc/hadoop subdirectories

## Cloudera Hadoop/HDFS Logs:

When YARN log aggregation is enabled: Container log files are moved from NodeManager hosts'
/var/log/hadoop-yarn/container directories to HDFS when the application completes
- Default HDFS directory: /tmp/logs
MapReduce jobs produce the following logs:
- ApplicationMaster log
- stdout, stderr, and syslog output for each Map and Reduce task
- Job configuration settings specified by the developer
- Counters
MapReduce Job History Server log directory Default: /var/log/hadoop-mapreduce

Spark application history location
– Default location in HDFS: /user/spark/applicationHistory
– Find and view the logs from the command line:
– $ sudo –u hdfs hdfs dfs /user/spark/applicationHistory
– $ sudo –u hdfs hdfs dfs –cat /user/spark/application_<application_id>/EVENT_LOG_1
# Spark History Server log directory Default: /var/log/spark

# Access all application logs in Cloudera Manager
– From the YARN Applications page, choose “Collect diagnostics data”
– Options to “Download Result Data”, and view recent or full logs

## Network troubleshooting commands:
tcpdump cheat sheet:
tcpdump [ -AdDefIKlLnNOpqRStuUvxX ] [ expression ]

sudo tcpdump -tttt -nn -l 'host scm1 and hadoop-01-01' # Only traffic between hosts scm1 and hadoop-01-01
sudo tcpdump -tttt -nn -l 'host hadoop-01-01 and ( port 24000 or 8022 or 22 )'
tcpdump 'tcp[tcpflags] & (tcp-syn|tcp-fin) != 0 and not src and dst net localnet' # To print the start and 
             end packets (the SYN and FIN packets) of each TCP conversation that involves a non-local host.
tcpdump -D # List device interfaces
tcpdump -i eth1 # Capture only on interface eth1
tcpdump -w file.pcap # Capture packets to file file.pcap
tcpdump -r file.cap  # Read from file file.pcap.
tcpdump -l  # Make stdout line buffered.  Useful if you want to see the data while capturing it.  E.g.,
               "tcpdump  -l  |  tee dat" or "tcpdump  -l   > dat  &  tail  -f  dat".
tcpdump -n  # Don't convert host addresses to names.  This can be used to avoid DNS lookups.
tcpdump -nn # Don't convert protocol and port numbers etc. to names either.
       -t     Don't print a timestamp on each dump line.
       -tt    Print an unformatted timestamp on each dump line.
       -ttt   Print a delta (micro-second resolution) between current and previous line on each dump line.
       -tttt  Print a timestamp in default format proceeded by date on each dump line.
       -ttttt Print a delta (micro-second resolution) between current and first line on each dump line.
e.g.   sudo tcpdump -tttt -l src hadoop-01-01 | tee /tmp/tcpdump1.txt
tcpdump -XX # Print packet contents in Hex and ASCII. Use -A for just ASCII

Filter Options  (see 'man pcap-filter'):
      type   type qualifiers say what kind of thing the id name or number refers to.  
             Possible types are host, net , port and portrange.  E.g., 'host foo', 'net 128.3', 'port 20', 'por-
              trange 6000-6008'.  If there is no type qualifier, host is assumed.
       dir    dir qualifiers specify a particular transfer direction to and/or from id.  
              Possible directions are src, dst, src or dst, src and dst,  etc.
              E.g.,  'src  foo',  'dst  net 128.3', 'src or dst port ftp-data'.  
              If there is no dir qualifier, src or dst is assumed.
       proto  proto  qualifiers restrict the match to a particular protocol.  Possible protos are: 
                 ether, fddi, tr, wlan, ip, ip6, arp, rarp, decnet, tcp and udp.  E.g., 'ether src foo',
              If there is no proto qualifier, all protocols consistent with the  type  are  assumed.
              E.g., 'src foo' means '(ip or arp or rarp) src foo' (except the latter is not legal syntax), and 
                    'port 53' means '(tcp or udp) port 53'.

      Primitives may be combined using: A parenthesized group of primitives and operators (parentheses are 
              special to the Shell and must be escaped).
              Negation ('!' or 'not'),  Concatenation ('&&' or 'and'),  Alternation ('||' or 'or').
       If an identifier is given without a keyword, the most recent keyword is assumed.  For example,
            not host vs and ace is short for not host vs and host ace

tcpdump Flags:
    [S] - SYN (Start Connection)
    [.] - No Flag Set
    [S.]- Some tcpdump versions print SYN-ACK as 'S.'
    [P] - PSH (Push Data)
    [F] - FIN (Finish Connection)
    [R] - RST (Reset Connection)

    When a TCP connection is being initialized, there is a three way handshake (SYN SYN-ACK ACK) to sync up the connection. 
    After the handshake, packets are sent and ACK along the way. When the connection is being terminated, there is another 
     three way handshake (FIN ACK-FIN ACK) to close the connection.`

=============================================================================

netcat/nc commands:

netcat -z -v domain.com 1-1000 # Use nc to scan ports 1-1000
netcat -z -n -v 111.111.111.111 1-1000 # Scan on a given IP address
nc -z -v hadoop-01-01 50000-65535 2>&1 | grep -v "Connection refused" # Filter out failed ports
nc -l 2517    # Listen on port 2517
nc <host> 2517   # Connect to host on port 2517


# Display network activity for a running process
strace -p <pid> -f -e trace=network -s 10000
